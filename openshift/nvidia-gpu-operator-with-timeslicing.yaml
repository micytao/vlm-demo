---
# NVIDIA GPU Operator Installation with Time-Slicing Configuration
# This file sets up the GPU Operator and enables GPU time-slicing for shared GPU resources
#
# Prerequisites:
# - OpenShift 4.x cluster
# - Node(s) with NVIDIA GPUs
# - Cluster admin privileges
#
# Usage:
#   oc apply -f nvidia-gpu-operator-with-timeslicing.yaml
#
# After applying, wait for all operator pods to be Running, then verify with:
#   oc get nodes -o json | jq '.items[].status.capacity | select(.["nvidia.com/gpu"] != null)'
#
# Expected result: nvidia.com/gpu should show "2" (or your configured replica count)

---
# Create namespace for GPU Operator
apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-gpu-operator

---
# Create OperatorGroup for GPU Operator
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator-group
  namespace: nvidia-gpu-operator
spec:
  targetNamespaces:
  - nvidia-gpu-operator

---
# Subscribe to NVIDIA GPU Operator
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gpu-operator-certified
  namespace: nvidia-gpu-operator
spec:
  channel: stable
  name: gpu-operator-certified
  source: certified-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Automatic

---
# ConfigMap for GPU Time-Slicing Configuration
# This configures each physical GPU to present as 2 virtual GPUs
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  any: |-
    version: v1
    flags:
      migStrategy: "none"
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 2

---
# ClusterPolicy to configure GPU Operator with time-slicing enabled
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
  labels:
    app.kubernetes.io/component: operators-instances
    app.kubernetes.io/name: gpu-operator
spec:
  # Operator configuration
  operator:
    defaultRuntime: crio
    use_ocp_driver_toolkit: true
    initContainer: {}

  # Driver configuration
  driver:
    enabled: true
    kernelModuleType: auto
    useNvidiaDriverCRD: false
    licensingConfig:
      nlsEnabled: true
      configMapName: ""
    certConfig:
      name: ""
    kernelModuleConfig:
      name: ""
    repoConfig:
      configMapName: ""
    virtualTopology:
      config: ""
    upgradePolicy:
      autoUpgrade: true
      maxUnavailable: "25%"
      maxParallelUpgrades: 1
      drain:
        enable: false
        force: false
        deleteEmptyDir: false
        timeoutSeconds: 300
      podDeletion:
        force: false
        deleteEmptyDir: false
        timeoutSeconds: 300
      waitForCompletion:
        timeoutSeconds: 0

  # Toolkit configuration
  toolkit:
    enabled: true

  # Device Plugin configuration with time-slicing enabled
  devicePlugin:
    enabled: true
    config:
      name: time-slicing-config
      default: any
    mps:
      root: /run/nvidia/mps

  # DCGM (Data Center GPU Manager)
  dcgm:
    enabled: true

  # DCGM Exporter for monitoring
  dcgmExporter:
    enabled: true
    config:
      name: ""
    serviceMonitor:
      enabled: true

  # GPU Feature Discovery
  gfd:
    enabled: true

  # MIG (Multi-Instance GPU) Manager
  migManager:
    enabled: true
  
  mig:
    strategy: single

  # Node Status Exporter
  nodeStatusExporter:
    enabled: true

  # Daemonset update strategy
  daemonsets:
    updateStrategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: "1"

  # Validator
  validator:
    plugin:
      env: []

  # Sandbox workloads
  sandboxWorkloads:
    enabled: false
    defaultWorkload: container

  # Sandbox device plugin
  sandboxDevicePlugin:
    enabled: true

  # VFIO Manager
  vfioManager:
    enabled: true

  # vGPU Device Manager
  vgpuDeviceManager:
    enabled: true

  # vGPU Manager
  vgpuManager:
    enabled: false

  # GDRCopy
  gdrcopy:
    enabled: false

  # GPUDirect Storage
  gds:
    enabled: false
